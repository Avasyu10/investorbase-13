import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2.39.3";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

export async function evaluateProblemStatementHandler(payload: any, authHeader?: string | null) {
  try {
    const { submission } = payload; // expects { submission } object with problem_statement
    const problem_statement = submission?.problem_statement;

    if (!problem_statement) throw new Error('Missing problem_statement in submission');

    const LOVABLE_API_KEY = Deno.env.get("LOVABLE_API_KEY");
    const SUPABASE_URL = Deno.env.get("SUPABASE_URL");
    const SUPABASE_SERVICE_ROLE_KEY = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY");

    // If LOVABLE_API_KEY is not configured, we'll run a deterministic mock evaluator
    const useMock = !LOVABLE_API_KEY;

    if (!authHeader) throw new Error('No authorization header');

    const supabase = createClient(SUPABASE_URL!, SUPABASE_SERVICE_ROLE_KEY!, { auth: { persistSession: false } });

    const token = authHeader.replace('Bearer ', '');
    const { data: { user }, error: userError } = await supabase.auth.getUser(token);
    if (userError || !user) throw new Error('Unauthorized');

    // Build system prompt with scoring rubric for Problem Statement only
    const systemPrompt = `You are a rigorous startup evaluator. Given a problem statement, score it on the following sub-criteria. For each sub-criterion return an integer score between 1 and 20. Provide also an analysis summary and 5 concise recommendations. Output must be a function call to provide_evaluation with a single JSON argument matching the schema.

Scoring sub-criteria for Problem Statement:
- existence: Does the problem exist? (1-20)
- severity: How severe is the problem? (1-20)
- frequency: How frequently does the problem occur? (1-20)
- unmet_need: Is there an unmet need? (1-20)

For each sub-criterion, follow the provided 1-20 rubrics (be conservative and justify scores in the analysis_summary). The analysis_summary should briefly explain the main drivers for the scores and call out key risks and strengths. recommendations should be 3-5 actionable bullets. Do NOT include any extra fields.
`;

    const userPrompt = `Problem Statement: ${problem_statement}`;

    let args: any = {};

    if (useMock) {
      // Deterministic mock evaluation based on problem_statement text
      console.warn('LOVABLE_API_KEY not set â€” running mock evaluation for development.');
      const seed = problem_statement || '';
      const hash = Array.from(seed).reduce((acc: number, ch: string) => (acc + ch.charCodeAt(0)) % 10000, 0);
      const pick = (base: number, offset: number) => {
        // produce a value 8-18 deterministically
        const v = 8 + ((hash + base + offset) % 11);
        return Math.max(1, Math.min(20, v));
      };

      args = {
        existence: pick(1, 3), severity: pick(2, 5), frequency: pick(3, 1), unmet_need: pick(4, 2),
        analysis_summary: `Mock evaluation (LOVABLE_API_KEY not configured). Generated deterministic scores based on problem statement content.`,
        recommendations: `Mock recommendations: Validate the problem with potential users; research severity and frequency; identify unmet needs.`
      };
    } else {
      const aiResponse = await fetch("https://ai.gateway.lovable.dev/v1/chat/completions", {
        method: "POST",
        headers: { Authorization: `Bearer ${LOVABLE_API_KEY}`, "Content-Type": "application/json" },
        body: JSON.stringify({
          model: "google/gemini-2.5-flash",
          messages: [{ role: 'system', content: systemPrompt }, { role: 'user', content: userPrompt }],
          tools: [{
            type: 'function',
            function: {
              name: 'provide_evaluation',
              description: 'Structured evaluation for problem statement',
              parameters: {
                type: 'object',
                properties: {
                  existence: { type: 'integer' }, severity: { type: 'integer' }, frequency: { type: 'integer' }, unmet_need: { type: 'integer' },
                  analysis_summary: { type: 'string' },
                  recommendations: { type: 'string' }
                },
                required: ['existence', 'severity', 'frequency', 'unmet_need', 'analysis_summary', 'recommendations'],
                additionalProperties: false
              }
            }
          }],
          tool_choice: { type: 'function', function: { name: 'provide_evaluation' } }
        })
      });

      if (!aiResponse.ok) {
        const errorText = await aiResponse.text();
        console.error('AI Gateway error:', aiResponse.status, errorText);
        throw new Error('AI Gateway error');
      }

      const aiData = await aiResponse.json();
      const toolCall = aiData.choices?.[0]?.message?.tool_calls?.[0];
      args = toolCall ? JSON.parse(toolCall.function.arguments) : {};
    }

    // Ensure integer bounds 1-20 for scores
    const clamp = (v: any) => {
      const n = parseInt(v);
      if (isNaN(n)) return 10;
      return Math.max(1, Math.min(20, n));
    };

    const record = {
      startup_name: null, // No full submission, set to null
      problem_statement: problem_statement,
      evaluator_user_id: user.id,
      existence_score: clamp(args.existence),
      severity_score: clamp(args.severity),
      frequency_score: clamp(args.frequency),
      unmet_need_score: clamp(args.unmet_need),
      // Set other scores to null or defaults since only problem statement is evaluated
      direct_fit_score: null,
      differentiation_score: null,
      feasibility_score: null,
      effectiveness_score: null,
      market_size_score: null,
      growth_trajectory_score: null,
      timing_readiness_score: null,
      external_catalysts_score: null,
      first_customers_score: null,
      accessibility_score: null,
      acquisition_approach_score: null,
      pain_recognition_score: null,
      direct_competitors_score: null,
      substitutes_score: null,
      differentiation_vs_players_score: null,
      dynamics_score: null,
      usp_clarity_score: null,
      usp_differentiation_strength_score: null,
      usp_defensibility_score: null,
      usp_alignment_score: null,
      tech_vision_ambition_score: null,
      tech_coherence_score: null,
      tech_alignment_score: null,
      tech_realism_score: null,
      tech_feasibility_score: null,
      tech_components_score: null,
      tech_complexity_awareness_score: null,
      tech_roadmap_score: null,
      overall_average: null,
      ai_analysis_summary: args.analysis_summary || null,
      ai_recommendations: args.recommendations || null
    };

    const { data: evaluation, error: dbError } = await supabase
      .from('submission_evaluations')
      .insert(record)
      .select()
      .single();

    if (dbError) {
      console.error('DB insert error:', dbError);
      throw new Error('Database error');
    }

    return new Response(JSON.stringify({ success: true, evaluation }), { headers: { ...corsHeaders, 'Content-Type': 'application/json' } });

  } catch (error) {
    console.error('Error in evaluate-problem-statement:', error);
    return new Response(JSON.stringify({ error: error instanceof Error ? error.message : 'Unknown error' }), { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } });
  }
}

// Serve wrapper uses the exported handler so other functions can import it for compatibility
serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  const payload = await req.json().catch(() => ({}));
  return await evaluateProblemStatementHandler(payload, req.headers.get('Authorization'));
});